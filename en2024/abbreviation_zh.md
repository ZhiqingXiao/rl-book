# 缩略语表

| 缩写 | 中文全称 | 英文全称 |
| :---: | --- | --- |
| A3C | 异步优势执行者/评论者算法 | Asynchronous Advantage Actor–Critic |
| AC | 执行者/评论者 | Actor–Critic |
| AI | 人工智能 | Artificial Intelligence |
| API | 应用编程接口 | Application Programming Interface |
| APV-MCTS | 异步策略价值回合更新树搜索 | Asynchronous Policy and Value MCTS |
| ARS | 增强随机搜索 | Augmented Random Search |
| BC | 行为克隆 | Behavior Cloning |
| CDF | 累积概率分布 | Cumulative Distribution Function |
| CG | 共轭梯度算法 | Conjugate Gradient |
| CNN | 卷积神经网络 | Convolutional Neural Network |
| CPU | 中央处理器 | Central Processing Unit |
| CTMDP | 连续时间Markov决策过程 | Continuous-Time Markov Decision Process |
| CTMP | 连续时间Markov过程 | Continuous-Time Markov Process |
| CTMRP | 连续时间Markov奖励过程 | Continuous-Time Markov Reward Process |
| CTPOMDP | 连续时间部分可观测Markov决策过程 | Continuous-Time Partially Observable Markov Decision Process |
| CTPOMP | 连续时间部分可观测Markov过程 | Continuous-Time Partially Observable Markov Process |
| CTPOMRP | 连续时间部分可观测Markov奖励过程 | Continuous-Time Partially Observable Markov Reward Process |
| CTSMDP | 连续时间半Markov决策过程 | Continuous-Time Semi-Markov Decision Process |
| CTSMP | 连续时间半Markov过程 | Continuous-Time Semi-Markov Process |
| CTSMRP | 连续时间半Markov奖励过程 | Continuous-Time Semi-Markov Reward Process |
| DDPG | 深度确定性策略梯度算法 | Deep Deterministic Policy Gradient |
| DL | 深度学习 | Deep Learning |
| DP | 动态规划 | Dynamic Programming |
| DPG | 确定性策略梯度算法 | Deterministic Policy Gradient |
| DQN | 深度Q网络 | Deep Q Network |
| DRL | 深度强化学习 | Deep Reinforcement Learning |
| DTMDP | 离散时间Markov决策过程 | Discrete-Time Markov Decision Process |
| DTMP | 离散时间Markov过程 | Discrete-Time Markov Process |
| DTMRP | 离散时间Markov奖励过程 | Discrete-Time Markov Reward Process |
| DTPOMDP | 离散时间部分可观测Markov决策过程 | Discrete-Time Partially Observable Markov Decision Process |
| DTPOMP | 离散时间部分可观测Markov过程 | Discrete-Time Partially Observable Markov Process |
| DTPOMRP | 离散时间部分可观测Markov奖励过程 | Discrete-Time Partially Observable Markov Reward Process |
| DTSMDP | 离散时间半Markov决策过程 | Discrete-Time Semi-Markov Decision Process |
| DTSMP | 离散时间半Markov过程 | Discrete-Time Semi-Markov Process |
| DTSMRP | 离散时间半Markov奖励过程 | Discrete-Time Semi-Markov Reward Process |
| ES | 进化策略 | Evolution Strategy |
| FIM | Fisher信息矩阵 | Fisher Information Matrix |
| GAIL | 生成对抗模仿学习 | Generative Adversarial Imitation Learning |
| GAE | 推广的优势估计 | Generalized Advantage Estimate |
| GAN | 生成对抗网络 | Generative Adversarial Network |
| GP | Gaussian过程 | Gaussian Process |
| GPT | 生成性预变换模型 | Generative Pre-trained Transformer |
| GPU | 图形处理器 | Graphics Processing Unit |
| HRL | 分层强化学习 | Hierarchical Reinforcement Learning |
| IL | 模仿学习 | Imitation Learning |
| IQN | 含蓄分位网络 | Implicit Quantile Networks |
| IRL | 逆强化学习 | Inverse Reinforcement Learning |
| JSD | Jensen-Shannon散度 | Jensen-Shannon Divergence |
| KLD | Kullback–Leibler散度 | Kullback–Leibler Divergence |
| MAB | 多臂赌博机 | Multi-Arm Bandit |
| MARL | 多智能体强化学习 | Multi-Agent Reinforcement Learning |
| MC | 回合制的 | Monte Carlo |
| MCTS | 回合更新树搜索 | Monte Carlo Tree Search |
| MDP | Markov决策过程 | Markov Decision Process |
| ML | 机器学习 | Machine Learning |
| MP | Markov过程 | Markov Process |
| MSE | 均方误差 | Mean Squared Error |
| MRP | Markov奖励过程 | Markov Reward Process |
| MTRL | 多任务强化学习 | Multi-Task Reinforcement Learning |
| NPG | 自然策略梯度算法 | Natural Policy Gradient |
| OffPAC | 异策的执行者/评论者算法 | Off-Policy Actor–Critic |
| OPDAC | 异策确定性执行者/评论者算法 | Off-Policy Deterministic Actor–Critic |
| OU | Ornstein Uhlenbeck过程 | Ornstein Uhlenbeck |
| PbRL | 偏好强化学习 | Preference-based Reinforcement Learning |
| PBVI | 点的价值迭代算法 | Point-Based Value Iteration |
| PDF | 概率分布函数 | Probability Distribution Function |
| PER | 优先经验回放 | Prioritized Experience Replay |
| PG | 策略梯度 | Policy Gradient |
| PMF | 概率质量函数 | Probability Mass Function |
| POMDP | 部分可观测Markov决策过程 | Partially Observable Markov Decision Process |
| POMP | 部分可观测Markov过程 | Partially Observable Markov Process |
| POMRP | 部分可观测Markov奖励过程 | Partially Observable Markov Reward Process |
| PPO | 邻近策略优化 | Proximal Policy Optimization |
| PUCT | 用于树的预测置信上界 | Predictor-Upper Confidence bounds applied to Trees |
| QF | 分位函数 | Quantile Function |
| QR | 分位数回归 | Quantile Regression |
| QR-DQN | 分位数回归深度Q网络 | Quantile Regression Deep Q Network |
| RAM | 随机存取存储器 | Random Access Memory |
| ReLU | 修正线性单元 | Rectified Linear Unit |
| RL | 强化学习 | Reinforcement Learning |
| RLHF | 人类反馈强化学习 | Reinforcement Learning with Human Feedback |
| RM | 奖励模型 | Reward Model |
| SAC | 柔性执行者/评论者算法 | Soft Actor–Critic |
| SARSA | 状态/动作/奖励/状态/动作 | State-Action-Reward-State-Action |
| SGD | 随机梯度下降 | Stochastic Gradient Descent  |
| SMDP | 半Markov决策过程 | Semi-Markov Decision Process |
| SMP | 半Markov过程 | Semi-Markov Process |
| SMRP | 半Markov奖励过程 | Semi-Markov Reward Process |
| SOTA | 最优方案 | State-Of-The-Art |
| SQL | 柔性Q学习 | Soft Q Learning |
| TD | 时序差分 | Temporal Difference |
| TD3 | 双重延迟深度确定性策略梯度算法 | Twin Delay Deep Deterministic Policy Gradient |
| TPU | 张量处理器 | Tensor Processing Unit |
| TRM | 信赖域方法 | Trust Region Method |
| TRPO | 信赖域策略优化算法 | Trust Region Policy Optimization |
| TV | 全变差 | Total Variation |
| UCB | 置信上界 | Upper Confidence Bound |
| UCB1 | 第一置信上界 | Upper Confidence Bound 1 |
| UCBVI | 置信上界价值迭代算法 | Upper Confidence Bound Value Iteration |
| UCT | 树的置信上界算法 | Upper Confidence bounds applied to Trees |
| VI | 价值迭代 | Value Iteration |
| VNM | von Neumann Morgenstern效用 | von Neumann Morgenstern |
| VPG | 简单的策略梯度算法 | Vanilla Policy Gradient |